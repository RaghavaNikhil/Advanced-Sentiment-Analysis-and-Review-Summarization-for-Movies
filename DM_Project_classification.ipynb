{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLynEWx6NEiK"
      },
      "source": [
        "# Imports and Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Uo-KT0cVgQ",
        "outputId": "362c3516-275e-44c0-df55-83249e2d9cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#To connect google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0k-9ri7JaAX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import string\n",
        "\n",
        "import html\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "import os.path\n",
        "import pandas as pd\n",
        "from math import inf\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import statistics as st\n",
        "from tabulate import tabulate\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import KFold,train_test_split\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "#NN\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "seed=22\n",
        "def reset_seeds(seed=seed):\n",
        "  keras.utils.set_random_seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  tf.keras.utils.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.config.experimental.enable_op_determinism()\n",
        "  os.environ['PYTHONHASHSEED']=str(seed)\n",
        "reset_seeds()\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "!pip install datasets\n",
        "import datasets\n",
        "\n",
        "# !pip install sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "clear_output()\n",
        "# !pip install nltk\n",
        "# !pip install pandas\n",
        "# !pip install tabulate\n",
        "# !pip install matplotlib\n",
        "# !pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaEX8IYBJZSN"
      },
      "outputs": [],
      "source": [
        "# Read input data\n",
        "# Input Data\n",
        "\n",
        "# Original\n",
        "# rt_reviews_data = pd.read_csv('https://drive.usercontent.google.com/download?id={}&export=download&authuser=0&confirm=t'.format(\"https://drive.google.com/file/d/1c4TpxkivElrGtgO2LjWCSyz3-aSNjwba/view?usp=sharing\".split('/')[-2]))\n",
        "\n",
        "#Processed\n",
        "# rt_reviews_data = pd.read_csv('https://drive.usercontent.google.com/download?id={}&export=download&authuser=0&confirm=t'.format(\"https://drive.google.com/file/d/1QXAL86wHtvN007STlBsiPipi2iOFBfEv/view?usp=sharing\".split('/')[-2]))\n",
        "\n",
        "#Sampled\n",
        "# rt_reviews_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRZG2c2f_7hJlc2RnQG-T08a6wU0fmdxTWpMh8jQB208PpACwybE8iVN2KHE_2zCx-Tj0IFXgPIgcT1/pub?gid=1388491722&single=true&output=csv')\n",
        "\n",
        "# rt_movies_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRa2reWtTa4XZ6QsfxLn1AQ9vvauK4UzAcg6ByF2iK0zhQkT_1KpdcXYnvMAJBToXM77qD9vicQ49lG/pub?gid=2111421593&single=true&output=csv')\n",
        "\n",
        "#IMDB\n",
        "rt_reviews_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSaniQnmQjFbEPc9-TDwvg0GnSvl0ij_VP-lEM9agQDBQUEFeZ0jzpl6kOCP1jCWhett1WsgDt6NNKU/pub?gid=683699041&single=true&output=csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MFmwJT-1adg",
        "outputId": "72edc74f-9603-4dfc-eda2-0a6ff935a6e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d53c3519-d506-4601-b816-b14a83288a14\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d53c3519-d506-4601-b816-b14a83288a14')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d53c3519-d506-4601-b816-b14a83288a14 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d53c3519-d506-4601-b816-b14a83288a14');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-772e8a8e-3696-456c-8d87-75321062d16b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-772e8a8e-3696-456c-8d87-75321062d16b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-772e8a8e-3696-456c-8d87-75321062d16b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rt_reviews_data",
              "summary": "{\n  \"name\": \"rt_reviews_data\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49581,\n        \"samples\": [\n          \"Poorly done political actioner. Badly photographed, acted, and directed. Every single scene is underlighted, including those very few that are shot during the daytime. It doesn't matter what the location is. At an important conference in the White House, no lights are on, and the only available lighting is a gloomy blue that is filtered through a few windows. The primier of China conducts an earth-shattering phone conversation under conditions of such intense chiaroscuro that he should be contemplating a bust of Homer in a Rembrandt painting. Honest. It's as if he had a tiny spotlight on his face and was otherwise in total darkness. The slow motion deaths are by now obligatory in any ill-thought-out movie.<br /><br />Roy Scheider and Maria Conchita Alonzo do well by their roles, but Scheider is rarely on screen. The other performances are dismissable. There is a pretty Oriental woman in a short tight skirt who totes a gun and is right out of a Bond movie who's accent suggests a childhood spent in Basset, Nebraska, and who should have remained the model she probably started out as. Whoever plays the surviving Secret Service agent aboard the cruise ship was probably picked for the part because he looked most like Johnny Depp, not because of any display of talent. The Chinese villains, representing both Taiwan and mainland China, hiss and grin as they threaten the heroes. <br /><br />The script is pretty awful, recycled from other, better films. There is a lot of shooting aboard the ship and practically everyone winds up mincemeat. Two thirds of the way through, the ship explodes into the expected series of fireballs. Then the movie splits into two related parts. Part one, another shootout, this time in a waterfront warehouse. Part two, an exchange between the Vice President, now acting president, and the oily Chinese premiere, lifted out of both \\\"Dr. Strangelove\\\" and \\\"Fail Safe.\\\" We unwittingly launch our missiles. They launch theirs in retaliation. We cannot convince them that our launch was accidental, even though we offer to help them destroy our own missiles. There is even the George C. Scott/ Walter Matthau general who argues that their \\\"nucular\\\" armory can't match ours so we should hit them with everything we've got. More fireballs. <br /><br />The end comes none too soon.\",\n          \"In Sri Lanka, a country divided by religion and language, the civil war between the pro-Sinhalese government and the Liberation Tigers of Tamil Eelam (LTTE), a separatist organization, has claimed an estimated 68,000 lives since 1983. Human rights groups have said that, as a result of the war, more than one million people have been displaced, homeless or living in camps. The impact on children and families caught in the conflict is sensitively dramatized by acclaimed Tamil director Mani Ratnam in his 2002 film A Peck on the Cheek, winner of several awards at the National Film Awards in India. While the civil war is merely a backdrop for the story of a young girl's voyage of discovery, the human cost of war is made quite clear and Ratnam gives the fighting a universal context, pointing the finger at global arms traffickers as the source of wrongdoing.<br /><br />Beautifully photographed in Southern India by cinematographer Ravi K Chandran in a setting mirroring the terrain of Sri Lanka, the film tells a moving story about an adopted 9-year old girl who sets out to find her real mother in the middle of the fighting in Sri Lanka. Played with deep feeling and expressiveness by P.S. Keerthana in a memorable performance, Amudha is brought up by a loving middle class family with two younger brothers after her natural parents Shyama (Nandita Das) and Dileepan (J.D. Chakravarthi) were forced to flee when the fighting broke out, leaving her in a Red Cross camp. In a loving flashback, we see Amudha's adoptive parents, father Thiru (Madhavan) a prominent Tamil writer, and mother Indra (Simran) a TV personality, marry to facilitate their adoption of the darker-skinned little girl.<br /><br />Young Amudha has no idea that she is adopted until it is sprung upon her abruptly on her ninth birthday, according to the parents' prior agreement. While she is playing, Thiru tells her almost in a matter of fact tone that \\\"you are not our daughter\\\" and the response is predictable. Distraught, she questions who her father was, what her mother's name was, why she gave her up, and so forth but few answers are forthcoming. Amudha runs away several times until her parents agree to go to Sri Lanka to help her find her true mother, now a fighter for the Tamil separatists. The family's immersion in the reality of the civil war leads to some traumatic moments and difficult decisions, handled mostly with skill by Ratnam, though a sequence where the family was caught in a crossfire felt amateurish.<br /><br />A Peck on the Cheek is of course a Bollywood-style film and that means tons of music and melodrama. The melodrama did not get in the way because of the strong performances by the lead actors; however, I found the musical dramatizations of songs by A. R. Rahman counter to the mood of the film with their slick, high production techniques and fast-paced music video-style editing. Yet the compelling nature of the story and the honesty in which it is told transcend the film's limitations. Tamil cinema has been criticized by many, even within the country as being too clich\\u00e9d and commercial, yet A Peck on the Cheek is both a film of entertainment and one that tackles serious issues. That it successfully straddles the line between art and commerce is not a rejection but a tribute.\",\n          \"FUTZ is the only show preserved from the experimental theatre movement in New York in the 1960s (the origins of Off Off Broadway). Though it's not for everyone, it is a genuinely brilliant, darkly funny, even more often deeply disturbing tale about love, sex, personal liberty, and revenge, a serious morality tale even more relevant now in a time when Congress wants to outlaw gay marriage by trashing our Constitution. The story is not about being gay, though -- it's about love and sex that don't conform to social norms and therefore must be removed through violence and hate. On the surface, it tells the story of a man who falls in love with a pig, but like any great fable, it's not really about animals, it's about something bigger -- stifling conformity in America.<br /><br />The stage version won international acclaim in its original production, it toured the U.S. and Europe, and with others of its kind, influenced almost all theatre that came after it. Luckily, we have preserved here the show pretty much as it was originally conceived, with the original cast and original director, Tom O'Horgan (who also directed HAIR and Jesus Christ Superstar on Broadway).<br /><br />This is not a mainstream, easy-to-take, studio film -- this is an aggressive, unsettling, glorious, deeply emotional, wildly imaginative piece of storytelling that you'll never forget. And it just might change the way you see the world...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "rt_reviews_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuVH0bQyBQXj"
      },
      "outputs": [],
      "source": [
        "keykey='rt'\n",
        "if('review' in rt_reviews_data.keys()):\n",
        "    keykey='imdb'\n",
        "    rt_reviews_data['reviewText']=rt_reviews_data['review']\n",
        "    rt_reviews_data['scoreSentiment']=rt_reviews_data['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26QkGoomrxhO"
      },
      "outputs": [],
      "source": [
        "# Subset of Original data\n",
        "rt_input_data = rt_reviews_data[['reviewText','scoreSentiment']]\n",
        "\n",
        "# Remove Null columns\n",
        "rt_input_data = rt_input_data.dropna(subset=['reviewText', 'scoreSentiment'])\n",
        "\n",
        "# Convert html symbols into text\n",
        "rt_input_data['reviewText'] = rt_input_data['reviewText'].apply(lambda x: html.unescape(x))\n",
        "\n",
        "# Save processed data for future reuse.\n",
        "# rt_input_data.to_csv('/content/drive/MyDrive/DO NOT DELETE/SHARE/rt_reviews_data_processed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4AzI0g2qGqT"
      },
      "outputs": [],
      "source": [
        "# rt_X_train, rt_X_test, rt_y_train, rt_y_test = train_test_split(rt_input_data['reviewText'], rt_input_data['scoreSentiment'], random_state=seed, test_size=0.25, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLwt70l5ZeyW",
        "outputId": "f5976aaa-13cc-4968-c87d-9d418bb5d896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdb\n"
          ]
        }
      ],
      "source": [
        "print(keykey)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Q5rFNBYYNf"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCNwxieEYopR"
      },
      "source": [
        "### Glove model utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc2zThmWjSoo"
      },
      "outputs": [],
      "source": [
        "# Create custom Vectorizer using glove model\n",
        "\n",
        "class Word2VecVectorizer:\n",
        "  def __init__(self, model):\n",
        "    # print(\"Loading in word vectors...\")\n",
        "    self.word_vectors = model\n",
        "    # print(\"Finished loading in word vectors\")\n",
        "\n",
        "  def fit(self, data):\n",
        "    pass\n",
        "\n",
        "  def transform(self, data):\n",
        "    # determine the dimensionality of vectors\n",
        "    v = self.word_vectors.get_vector('king')\n",
        "    self.D = v.shape[0]\n",
        "\n",
        "    X = np.zeros((len(data), self.D))\n",
        "    n = 0\n",
        "    emptycount = 0\n",
        "    for sentence in data:\n",
        "      tokens = sentence.split()\n",
        "      vecs = []\n",
        "      m = 0\n",
        "      for word in tokens:\n",
        "        try:\n",
        "          # throws KeyError if word not found\n",
        "          vec = self.word_vectors.get_vector(word)\n",
        "          vecs.append(vec)\n",
        "          m += 1\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if len(vecs) > 0:\n",
        "        vecs = np.array(vecs)\n",
        "        X[n] = vecs.mean(axis=0)\n",
        "      else:\n",
        "        emptycount += 1\n",
        "      n += 1\n",
        "    # print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
        "    return X\n",
        "\n",
        "\n",
        "  def fit_transform(self, data):\n",
        "    self.fit(data)\n",
        "    return self.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrEw4_Rgl_Zd"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using glove model\n",
        "glove_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/glove_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/glove_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        glove_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    # load the Stanford GloVe model\n",
        "    glove_path='/content/drive/MyDrive/glove.6B.100d.txt' #Colab\n",
        "    # glove_path='./glove.6B.100d.txt' #Local\n",
        "    glove_model_data = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
        "    def X_tensor_glove_original(corpus):\n",
        "        corpus=corpus.copy()\n",
        "        vectorizer = Word2VecVectorizer(glove_model_data)\n",
        "        X_train = vectorizer.fit_transform(corpus)\n",
        "        X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        return X_train\n",
        "\n",
        "    input_converted_glove=X_tensor_glove_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    glove_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        glove_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_glove[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/glove_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(glove_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEhrPeIlbRq"
      },
      "source": [
        "### bert model utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7lrafUOSIfP"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using all-distilroberta-v1 bert model\n",
        "bert_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/bert_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/bert_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        bert_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    # Load all-distilroberta-v1 model\n",
        "    bert_model = SentenceTransformer('all-distilroberta-v1')\n",
        "    clear_output()\n",
        "    def X_tensor_bert_original(corpus):\n",
        "        corpus=corpus.copy()\n",
        "        X_train = bert_model.encode(corpus)\n",
        "        X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        return X_train\n",
        "\n",
        "    input_converted_bert=X_tensor_bert_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    bert_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        bert_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_bert[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/bert_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(bert_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jCKm7eQyUrY"
      },
      "source": [
        "### mpnet model utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9icKvX5yTrx"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using all-mpnet-base-v2 mpnet model\n",
        "mpnet_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        mpnet_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    # Load all-mpnet-base-v2 model\n",
        "    mpnet_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "    clear_output()\n",
        "    def X_tensor_mpnet_original(corpus):\n",
        "        corpus=corpus.copy()\n",
        "        X_train = mpnet_model.encode(corpus)\n",
        "        X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        return X_train\n",
        "\n",
        "    input_converted_mpnet=X_tensor_mpnet_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    mpnet_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        mpnet_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_mpnet[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(mpnet_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADnnvdZ2KMc3"
      },
      "source": [
        "### Count Vectorizer utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "art1xsWXG-H3"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using countvec model\n",
        "countvec_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/countvec_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/countvec_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        countvec_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    class LemmaTokenizer:\n",
        "      def __init__(self):\n",
        "          self.wnl = WordNetLemmatizer()\n",
        "      def __call__(self, doc):\n",
        "          return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "    def X_tensor_countvec_original(corpus):\n",
        "      corpus=corpus.copy()\n",
        "      vectorizer = CountVectorizer(max_features=1000, stop_words='english', lowercase=True, tokenizer=LemmaTokenizer())\n",
        "      X_train = vectorizer.fit_transform(corpus)\n",
        "      X_train=X_train.toarray()\n",
        "      X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "      return X_train\n",
        "\n",
        "    input_converted_countvec=X_tensor_countvec_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    countvec_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        countvec_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_countvec[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/countvec_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(countvec_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mjnOwK8KNsv"
      },
      "source": [
        "### TFIDF utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzdGtQUmJW4l"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using tfidf model\n",
        "tfidf_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/tfidf_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/tfidf_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        tfidf_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    class LemmaTokenizer:\n",
        "      def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "      def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "    def X_tensor_tfidf_original(corpus):\n",
        "      corpus=corpus.copy()\n",
        "      vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', lowercase=True, tokenizer=LemmaTokenizer())\n",
        "      X_train = vectorizer.fit_transform(corpus)\n",
        "      X_train=X_train.toarray()\n",
        "      X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "      return X_train\n",
        "\n",
        "    input_converted_tfidf=X_tensor_tfidf_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    tfidf_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        tfidf_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_tfidf[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/tfidf_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(tfidf_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP44VjJ4lhfA"
      },
      "source": [
        "### General Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoaWzqJCgNAf",
        "outputId": "cf6cd508-6163-4a3f-f716-5ac2de07b7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'negative': [1, 0], 'positive': [0, 1]}\n",
            "{0: 'negative', 1: 'positive'}\n"
          ]
        }
      ],
      "source": [
        "# Create Dictionary to convert category into 1-hot vector and vice verse\n",
        "category_to_vector={}\n",
        "vector_to_category={}\n",
        "\n",
        "index=0\n",
        "set_of_list_categories=set(list(rt_reviews_data['scoreSentiment']))\n",
        "set(list(rt_reviews_data['scoreSentiment']))\n",
        "for i in list(set_of_list_categories):\n",
        "  newarray=[0]*len(set_of_list_categories)\n",
        "  newarray[index]=1\n",
        "  category_to_vector[i]=newarray\n",
        "  vector_to_category[index]=i\n",
        "  index+=1\n",
        "\n",
        "print(category_to_vector)\n",
        "print(vector_to_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2tgRkBEW9E5"
      },
      "outputs": [],
      "source": [
        "# utility functions to convert category into 1-hot vector and vice verse\n",
        "def Y_tensor(Y_train):\n",
        "  Y_train=Y_train.copy()\n",
        "\n",
        "  for i in range(len(Y_train)):\n",
        "    Y_train[i]=category_to_vector[Y_train[i]]\n",
        "\n",
        "  Y_train=np.array(Y_train)\n",
        "  Y_train=tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "  return Y_train\n",
        "\n",
        "def vec2cat(input):\n",
        "    categories = []\n",
        "    for i in range(len(input)):\n",
        "        categories.append(vector_to_category[np.argmax(input[i])])\n",
        "    return categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awEeTeYtbRhI"
      },
      "outputs": [],
      "source": [
        "# Neural Network Model\n",
        "def generateNNModel(learning_rate, input_dim, optimizer_name):\n",
        "  reset_seeds()\n",
        "  model = Sequential()\n",
        "  model.add(keras.Input(shape=(input_dim,)))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(len(set_of_list_categories), activation='softmax'))\n",
        "  optimzer=None\n",
        "  if optimizer_name=='adam':\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  elif optimizer_name=='sgd':\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "  elif optimizer_name=='rmsprop':\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix4hv0zgj3BX"
      },
      "outputs": [],
      "source": [
        "# initialize array to save the various features statistics\n",
        "all_train_acc=[]\n",
        "all_train_std=[]\n",
        "all_train_f1=[]\n",
        "all_train_auc=[]\n",
        "\n",
        "all_val_acc=[]\n",
        "all_val_std=[]\n",
        "all_val_f1=[]\n",
        "all_val_auc=[]\n",
        "def reset_stat_array():\n",
        "  global all_train_acc,all_train_std,all_val_acc,all_val_std, all_train_f1, all_val_f1, all_train_auc, all_val_auc\n",
        "  all_train_acc=[]\n",
        "  all_train_std=[]\n",
        "  all_train_f1=[]\n",
        "  all_train_auc=[]\n",
        "\n",
        "  all_val_acc=[]\n",
        "  all_val_std=[]\n",
        "  all_val_f1=[]\n",
        "  all_val_auc=[]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHwafw5SbczO"
      },
      "outputs": [],
      "source": [
        "#A common function to run Neural Network on training data and test data. Uses arguments to change various parameters.\n",
        "def run_nn(X,Y, input_size, nn_epochs, filename_suffix, run_test_data, learning_rate, optimizer_name):\n",
        "    #Kfold\n",
        "    def kfold_analysis(train_val_X,train_val_y):\n",
        "        kf = KFold(n_splits = 5)\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "\n",
        "        train_f1 = []\n",
        "        val_f1 = []\n",
        "\n",
        "        train_auc = []\n",
        "        val_auc = []\n",
        "        for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "            train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "            train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "            val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "            val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "            # train_X_tensor=X_tensor(train_X)\n",
        "            #train\n",
        "            nn_model = generateNNModel(learning_rate, input_size,optimizer_name)\n",
        "            reset_seeds()\n",
        "            train_X=tf.convert_to_tensor(train_X, dtype=tf.float32)\n",
        "            history = nn_model.fit(train_X, Y_tensor(train_y), epochs=nn_epochs, batch_size=64,verbose=0)\n",
        "\n",
        "            #accuracy of training\n",
        "            reset_seeds()\n",
        "            training_data_predicted_values=nn_model.predict(train_X,verbose=0)\n",
        "            # print(training_data_predicted_values)\n",
        "            training_data_predicted_categories = vec2cat(training_data_predicted_values)\n",
        "\n",
        "            # print(set(train_y), set(training_data_predicted_categories))\n",
        "            train_acc.append(accuracy_score(train_y, training_data_predicted_categories))\n",
        "            train_f1.append(f1_score(train_y, training_data_predicted_categories, pos_label='positive'))\n",
        "            train_auc.append(roc_auc_score(train_y, training_data_predicted_values[:, 0]))\n",
        "\n",
        "            #accuracy of validation\n",
        "            reset_seeds()\n",
        "            val_X=tf.convert_to_tensor(val_X, dtype=tf.float32)\n",
        "            validation_data_predicted_values=nn_model.predict(val_X,verbose=0)\n",
        "            validation_data_predicted_categories = vec2cat(validation_data_predicted_values)\n",
        "\n",
        "            # print(st(val_y), set(validation_data_predicted_categories))\n",
        "            val_acc.append(accuracy_score(val_y, validation_data_predicted_categories))\n",
        "            val_f1.append(f1_score(val_y, validation_data_predicted_categories, pos_label='positive'))\n",
        "            val_auc.append(roc_auc_score(val_y, validation_data_predicted_values[:, 0]))\n",
        "\n",
        "        avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "        avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "        avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "        avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "        avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "        avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "\n",
        "        print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "        print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "\n",
        "        return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc\n",
        "    return kfold_analysis(X,Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpsmP1gZYb5T"
      },
      "source": [
        "# Feature Extraction and NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP-JBJOfJwuU"
      },
      "source": [
        "### Utility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzIndmeXJv4G"
      },
      "outputs": [],
      "source": [
        "def X_tensor_countvec(corpus):\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(countvec_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "X_countvec=X_tensor_countvec(rt_reviews_data['reviewText'])\n",
        "Y=rt_reviews_data['scoreSentiment']\n",
        "\n",
        "\n",
        "\n",
        "def X_tensor_tfidf(corpus):\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(tfidf_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "\n",
        "X_tfidf=X_tensor_tfidf(rt_reviews_data['reviewText'])\n",
        "\n",
        "def X_tensor_glove(corpus):\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(glove_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "X_glove=X_tensor_glove(rt_reviews_data['reviewText'])\n",
        "\n",
        "\n",
        "def X_tensor_bert(corpus):\n",
        "  corpus=corpus.copy()\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(bert_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "X_bert=X_tensor_bert(rt_reviews_data['reviewText'])\n",
        "\n",
        "\n",
        "\n",
        "def X_tensor_mpnet(corpus):\n",
        "  corpus=corpus.copy()\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(mpnet_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "\n",
        "X_mpnet=X_tensor_mpnet(rt_reviews_data['reviewText'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw2IDx9Xyc7T"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXHAXtY0kKlf",
        "outputId": "757568fb-9b08-4046-f9d7-4ab6d392a4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [03:29, 41.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The average training accuracy is 0.9953660000000001 with standard deviation of 0.0020341838658292704\n",
            "The average validation accuracy is 0.9327799999999999 with standard deviation of 0.0771889733057773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run Neural Network using Count Vectorizer feature\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_countvec),list(Y), 1000, 10, 'countvec', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr0BpG5iyamL"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdIjeZWmsXn3",
        "outputId": "90d255ff-7738-40dd-ebc5-d29d1008782c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [04:12, 50.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The average training accuracy is 0.9974230000000001 with standard deviation of 0.000801708176333501\n",
            "The average validation accuracy is 0.9356 with standard deviation of 0.07499485315673338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run Neural Network using TFIDF Vectorizer feature\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_tfidf),list(Y), 1000, 15, '_tfidf', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi1prRnGyYNr"
      },
      "source": [
        "### Glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU9alRnltNnD",
        "outputId": "54ee27a1-a21a-4a01-e834-562d08924edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [11:27, 137.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The average training accuracy is 0.811975 with standard deviation of 0.003188259713385961\n",
            "The average validation accuracy is 0.8049 with standard deviation of 0.009048977842828449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run Neural Network using Glove Vectorizer feature\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_glove),list(Y), 100, 50, '_glove', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7jOY_RNlIw5"
      },
      "source": [
        "### Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1vZfudulA7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6a541f-313c-4501-fa70-3d9fae94cfe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [03:59, 47.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The average training accuracy is 0.9793329999999999 with standard deviation of 0.004668937352331897\n",
            "The average validation accuracy is 0.9363400000000001 with standard deviation of 0.04867815115634528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run Neural Network using BERT Vectorizer feature\n",
        "dimension_sbert=768\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_bert),list(Y), dimension_sbert, 15, '_bert', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITc-IDpHdjwC"
      },
      "source": [
        "### MPNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdz4_Ulg1clG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac3d6a5-8dbf-4bc9-890a-35486f15a636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [04:19, 51.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The average training accuracy is 0.9836050000000001 with standard deviation of 0.004466049708635156\n",
            "The average validation accuracy is 0.94666 with standard deviation of 0.04057509581011485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run Neural Network using MPnet Vectorizer feature\n",
        "dimension_mpnet=768\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_mpnet),list(Y), dimension_mpnet, 15, '_mpnet', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcZVpGGpXFJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b73d65-bcc3-4f1f-c11e-6c09e9041990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------+------------+-----------+\n",
            "| Method           |   Accuracy |   F1 Score |       AUC |\n",
            "+==================+============+============+===========+\n",
            "| Count Vectorizer |    0.93278 |   0.933421 | 0.034396  |\n",
            "+------------------+------------+------------+-----------+\n",
            "| TFIDF            |    0.9356  |   0.936042 | 0.0338772 |\n",
            "+------------------+------------+------------+-----------+\n",
            "| GloVe            |    0.8049  |   0.807359 | 0.109673  |\n",
            "+------------------+------------+------------+-----------+\n",
            "| BERT             |    0.93634 |   0.936714 | 0.0237806 |\n",
            "+------------------+------------+------------+-----------+\n",
            "| MPNET            |    0.94666 |   0.946851 | 0.0202529 |\n",
            "+------------------+------------+------------+-----------+\n"
          ]
        }
      ],
      "source": [
        "# # Create a table to compare the statistics\n",
        "# def drawTable(all_train_acc,all_val_acc,all_train_std,all_val_std):\n",
        "#     table={\"Method\":['Count Vectorizer','TFIDF','GloVe','BERT','MPNET'], \"Training Accuracy\":all_train_acc, \"Validation accuracy\":all_val_acc, \"Training Acc. Deviation\":all_train_std, \"Validation Acc. Deviation\":all_val_std}\n",
        "#     print(tabulate(table , headers=\"keys\", tablefmt=\"grid\"))\n",
        "\n",
        "# drawTable(all_train_acc,all_val_acc,all_train_std,all_val_std)\n",
        "\n",
        "# Create a table to compare the statistics\n",
        "def drawTable(all_val_acc,all_val_f1,all_val_auc):\n",
        "    table={\"Method\":['Count Vectorizer','TFIDF','GloVe','BERT','MPNET'],\"Accuracy\":all_val_acc,\"F1 Score\":all_val_f1,\"AUC\":all_val_auc}\n",
        "    print(tabulate(table , headers=\"keys\", tablefmt=\"grid\"))\n",
        "\n",
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDibaBLdbMb3"
      },
      "outputs": [],
      "source": [
        "# #Bar chart showing the training accuracy and validation accuracy w.r.t. different parameter values\n",
        "# labels = (\"Training Accuracy\", \"Validation Accuracy\")\n",
        "# pdata_means = {\n",
        "#     'Count Vectorizer': (all_train_acc[0], all_val_acc[0]),\n",
        "#     'TFIDF': (all_train_acc[1], all_val_acc[1]),\n",
        "#     'GloVe': (all_train_acc[2], all_val_acc[2]),\n",
        "#     'BERT': (all_train_acc[3], all_val_acc[3]),\n",
        "#     'MPNET': (all_train_acc[4], all_val_acc[4]),\n",
        "# }\n",
        "\n",
        "# x = np.arange(len(labels))  # the label locations\n",
        "# width = 0.10  # the width of the bars\n",
        "# multiplier = 0\n",
        "\n",
        "# fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "# for attribute, measurement in pdata_means.items():\n",
        "#     print(attribute,measurement)\n",
        "#     offset = width * multiplier\n",
        "#     rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
        "#     ax.bar_label(rects, padding=3)\n",
        "#     multiplier += 1\n",
        "\n",
        "\n",
        "# ax.set_ylabel('Accuracy')\n",
        "# ax.set_title('Accuracy w.r.t Features')\n",
        "# ax.set_xticks(x + width, labels)\n",
        "# ax.legend(loc='best')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2SqBbnlYkDO"
      },
      "source": [
        "# Other Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ36Yzvn7JUB"
      },
      "source": [
        "### RandomForests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekaaK4lB9Ey1"
      },
      "outputs": [],
      "source": [
        "reset_stat_array()\n",
        "def randomForestsRun(train_val_X, train_val_y):\n",
        "\n",
        "    kf = KFold(n_splits = 5)\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    train_f1 = []\n",
        "    val_f1 = []\n",
        "\n",
        "    train_auc = []\n",
        "    val_auc = []\n",
        "\n",
        "    for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "        train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "        train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "        val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "        val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "        dtc = RandomForestClassifier(n_estimators=128, min_samples_leaf=1, max_features=10, random_state=seed)\n",
        "        dtc.fit(train_X, train_y)\n",
        "        train_acc.append(dtc.score(train_X, train_y))\n",
        "        train_f1.append(f1_score(train_y, dtc.predict(train_X), pos_label='positive'))\n",
        "        train_auc.append(roc_auc_score(train_y, dtc.predict_proba(train_X)[:, 1]))\n",
        "\n",
        "        val_acc.append(dtc.score(val_X, val_y))\n",
        "        val_f1.append(f1_score(val_y, dtc.predict(val_X), pos_label='positive'))\n",
        "        val_auc.append(roc_auc_score(val_y, dtc.predict_proba(val_X)[:, 1]))\n",
        "\n",
        "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "    avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "    avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "    avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "    print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "    print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "    return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju5qpTKH7HHb",
        "outputId": "85bc41b6-db95-46f3-8424-d95d81be6157"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [03:51, 46.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 1.0 with standard deviation of 0.0\n",
            "The average validation accuracy is 0.93584 with standard deviation of 0.07861833373965642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [04:40, 56.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 1.0 with standard deviation of 0.0\n",
            "The average validation accuracy is 0.9366399999999999 with standard deviation of 0.07762302751117092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [07:01, 84.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.999905 with standard deviation of 9.999999999976694e-06\n",
            "The average validation accuracy is 0.90402 with standard deviation of 0.11747723864647143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [07:49, 93.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 1.0 with standard deviation of 0.0\n",
            "The average validation accuracy is 0.9339599999999999 with standard deviation of 0.08089305532615271\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [07:55, 95.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 1.0 with standard deviation of 0.0\n",
            "The average validation accuracy is 0.94248 with standard deviation of 0.0704507601094552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Random Forests model using 5-fold cross-validation\n",
        "#Count\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_countvec,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#TFIDF\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_tfidf,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#Glove\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_glove,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#BERT\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_bert,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#MPNET\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_mpnet,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pubPaLAnYeFE",
        "outputId": "3ed45142-e5b6-476e-e014-93b57be4dda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------+------------+----------+\n",
            "| Method           |   Accuracy |   F1 Score |      AUC |\n",
            "+==================+============+============+==========+\n",
            "| Count Vectorizer |    0.93584 |   0.936163 | 0.96713  |\n",
            "+------------------+------------+------------+----------+\n",
            "| TFIDF            |    0.93664 |   0.936949 | 0.968511 |\n",
            "+------------------+------------+------------+----------+\n",
            "| GloVe            |    0.90402 |   0.904332 | 0.935672 |\n",
            "+------------------+------------+------------+----------+\n",
            "| BERT             |    0.93396 |   0.936149 | 0.96587  |\n",
            "+------------------+------------+------------+----------+\n",
            "| MPNET            |    0.94248 |   0.944295 | 0.973171 |\n",
            "+------------------+------------+------------+----------+\n"
          ]
        }
      ],
      "source": [
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Es_n4qXc9N"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRwuj8hG-LHe"
      },
      "outputs": [],
      "source": [
        "reset_stat_array()\n",
        "def naiveBayesRun(train_val_X, train_val_y):\n",
        "    kf = KFold(n_splits = 5)\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    train_f1 = []\n",
        "    val_f1 = []\n",
        "\n",
        "    train_auc = []\n",
        "    val_auc = []\n",
        "\n",
        "    for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "        train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "        train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "        val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "        val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "        gnb = GaussianNB()\n",
        "        gnb.fit(train_X, train_y)\n",
        "\n",
        "        train_acc.append(gnb.score(train_X, train_y))\n",
        "        train_f1.append(f1_score(train_y, gnb.predict(train_X), pos_label='positive'))\n",
        "        train_auc.append(roc_auc_score(train_y, gnb.predict_proba(train_X)[:, 1]))\n",
        "\n",
        "        val_acc.append(gnb.score(val_X, val_y))\n",
        "        val_f1.append(f1_score(val_y, gnb.predict(val_X), pos_label='positive'))\n",
        "        val_auc.append(roc_auc_score(val_y, gnb.predict_proba(val_X)[:, 1]))\n",
        "\n",
        "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "    avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "    avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "    avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "    print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "    print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "    return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmFNPrp2CtJm",
        "outputId": "da8f5ec6-4fea-484c-f3fd-489d39e79b15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:33,  6.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.8111029999999999 with standard deviation of 0.0015366378883783875\n",
            "The average validation accuracy is 0.80942 with standard deviation of 0.002413627974647275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:19,  3.95s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.814611 with standard deviation of 0.00043573386372878397\n",
            "The average validation accuracy is 0.8126599999999999 with standard deviation of 0.0034459251297728226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:04,  1.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.688775 with standard deviation of 0.0008666025617317279\n",
            "The average validation accuracy is 0.6883600000000001 with standard deviation of 0.002677760258126177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:15,  3.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.802257 with standard deviation of 0.0004917479028933617\n",
            "The average validation accuracy is 0.8020799999999999 with standard deviation of 0.003516475508232639\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:16,  3.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.818571 with standard deviation of 0.00026008460162032344\n",
            "The average validation accuracy is 0.8184400000000001 with standard deviation of 0.003189106457928287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Naive Bayes model using 5-fold cross-validation\n",
        "#Count\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_countvec,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#TFIDF\n",
        "X_tfidf=X_tensor_tfidf(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_tfidf,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#Glove\n",
        "X_glove=X_tensor_glove(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_glove,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#BERT\n",
        "X_bert=X_tensor_bert(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_bert,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#MPNET\n",
        "X_mpnet=X_tensor_mpnet(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_mpnet,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW3qvMuVC2dY",
        "outputId": "dd768aea-b601-40cb-c5c5-3db73028e9fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------+------------+----------+\n",
            "| Method           |   Accuracy |   F1 Score |      AUC |\n",
            "+==================+============+============+==========+\n",
            "| Count Vectorizer |    0.80942 |   0.807185 | 0.870049 |\n",
            "+------------------+------------+------------+----------+\n",
            "| TFIDF            |    0.81266 |   0.813679 | 0.868493 |\n",
            "+------------------+------------+------------+----------+\n",
            "| GloVe            |    0.68836 |   0.666702 | 0.760523 |\n",
            "+------------------+------------+------------+----------+\n",
            "| BERT             |    0.80208 |   0.809187 | 0.878999 |\n",
            "+------------------+------------+------------+----------+\n",
            "| MPNET            |    0.81844 |   0.824776 | 0.891235 |\n",
            "+------------------+------------+------------+----------+\n"
          ]
        }
      ],
      "source": [
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_9txbHXkJi"
      },
      "source": [
        "### SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JivnaUiBDFWn"
      },
      "outputs": [],
      "source": [
        "reset_stat_array()\n",
        "def sgdRun(train_val_X, train_val_y):\n",
        "\n",
        "    kf = KFold(n_splits = 5)\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    train_f1 = []\n",
        "    val_f1 = []\n",
        "\n",
        "    train_auc = []\n",
        "    val_auc = []\n",
        "\n",
        "    for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "        train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "        train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "        val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "        val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "        sgd = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))\n",
        "        sgd.fit(train_X, train_y)\n",
        "\n",
        "        train_acc.append(sgd.score(train_X, train_y))\n",
        "        train_f1.append(f1_score(train_y, sgd.predict(train_X), pos_label='positive'))\n",
        "        train_auc.append(roc_auc_score(train_y, sgd.decision_function(train_X)))\n",
        "\n",
        "        val_acc.append(sgd.score(val_X, val_y))\n",
        "        val_f1.append(f1_score(val_y, sgd.predict(val_X), pos_label='positive'))\n",
        "        val_auc.append(roc_auc_score(val_y, sgd.decision_function(val_X)))\n",
        "\n",
        "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "    avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "    avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "    avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "    print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "    print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "    return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIShy9x3E-FK",
        "outputId": "5860f971-5631-4892-b688-bc86495cdbbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [01:31, 18.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.851399 with standard deviation of 0.003766489612357928\n",
            "The average validation accuracy is 0.8462 with standard deviation of 0.004845616575834313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [01:37, 19.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.8510390000000001 with standard deviation of 0.0037274527495328395\n",
            "The average validation accuracy is 0.8448599999999999 with standard deviation of 0.008576852569561843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:12,  2.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.7830389999999999 with standard deviation of 0.002462731004393284\n",
            "The average validation accuracy is 0.78346 with standard deviation of 0.004319537012227101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [01:02, 12.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.8783429999999999 with standard deviation of 0.002884124823928393\n",
            "The average validation accuracy is 0.87514 with standard deviation of 0.0030571882506643245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [01:02, 12.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.8959820000000001 with standard deviation of 0.0024454459715969783\n",
            "The average validation accuracy is 0.8919 with standard deviation of 0.0019493588689617671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the SGD model using 5-fold cross-validation\n",
        "#Count\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_countvec,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#TFIDF\n",
        "X_tfidf=X_tensor_tfidf(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_tfidf,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#Glove\n",
        "X_glove=X_tensor_glove(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_glove,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#BERT\n",
        "X_bert=X_tensor_bert(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_bert,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#MPNET\n",
        "X_mpnet=X_tensor_mpnet(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_mpnet,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBKedlXpYQRd",
        "outputId": "a6e524dd-cd63-4ec0-d56f-9d9f8e0d394d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------+------------+----------+\n",
            "| Method           |   Accuracy |   F1 Score |      AUC |\n",
            "+==================+============+============+==========+\n",
            "| Count Vectorizer |    0.8462  |   0.8484   | 0.919194 |\n",
            "+------------------+------------+------------+----------+\n",
            "| TFIDF            |    0.84486 |   0.845736 | 0.922799 |\n",
            "+------------------+------------+------------+----------+\n",
            "| GloVe            |    0.78346 |   0.781913 | 0.860575 |\n",
            "+------------------+------------+------------+----------+\n",
            "| BERT             |    0.87514 |   0.876274 | 0.946516 |\n",
            "+------------------+------------+------------+----------+\n",
            "| MPNET            |    0.8919  |   0.89231  | 0.957796 |\n",
            "+------------------+------------+------------+----------+\n"
          ]
        }
      ],
      "source": [
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMw-IgLTOzvE"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s3kClKPu3rO",
        "outputId": "ad7c826f-989b-4286-f3ae-8692b6ecf23f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-distilroberta-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PspR-x8IQ98V"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "import datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(rt_reviews_data['reviewText'],rt_reviews_data['scoreSentiment'],test_size=0.2, shuffle=True, random_state=seed)\n",
        "\n",
        "dataset_train={'text':X_train, 'labels':[1 if x=='positive' else 0 for x in list(y_train)]}\n",
        "dataset_test={'text':X_test, 'labels':[1 if x=='positive' else 0 for x in list(y_test)]}\n",
        "\n",
        "dataset_train=datasets.Dataset.from_pandas(pd.DataFrame(data=dataset_train))\n",
        "dataset_test=datasets.Dataset.from_pandas(pd.DataFrame(data=dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "kBczt5HXPWDs",
        "outputId": "f63596b0-0d62-4b4b-e312-1da2d12fabfa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44eb252a5e304d2fa73f5f1e7e17eac7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e09272740984f3d81fe5f0a9fbcfba2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_train = dataset_train.map(preprocess_function, batched=True)\n",
        "tokenized_test = dataset_test.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5prh9IRDPi8F"
      },
      "outputs": [],
      "source": [
        "# tokenized_train = tokenized_train.remove_columns('text')\n",
        "# tokenized_test = tokenized_test.remove_columns('text')\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mcG1C9IPlMd"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForSequenceClassification\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJqR1FigPolM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOWZkMxLRxxH",
        "outputId": "7c84c107-78d9-4b1c-c7b0-32f1e20ce891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.29.2\n"
          ]
        }
      ],
      "source": [
        "# RUN this first, Comment it and then restart session and run all.\n",
        "# !pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnEu2YoYU0j5",
        "outputId": "cabf4431-634a-4f3e-d2ff-922004116c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n",
        "#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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8QrP-_LQVhf",
        "outputId": "bf44f6ab-54f4-4fe4-f167-541bf6ed6c99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "repo_name = \"finetuning-sentiment-model-distilbert-imdb\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "   output_dir=repo_name,\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=2,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Oq9LvWhAkkuZ",
        "outputId": "d88f7d7f-1387-41a2-c978-a10c3bcfb662"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 1:07:35, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.297800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.222700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.203400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.134400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.139200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.127600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.135800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.125600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.1768983871459961, metrics={'train_runtime': 4055.9249, 'train_samples_per_second': 19.724, 'train_steps_per_second': 1.233, 'total_flos': 1.0454799017039232e+16, 'train_loss': 0.1768983871459961, 'epoch': 2.0})"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "galNB1J-ki_G",
        "outputId": "fd7f5746-544a-4dfd-8d41-4b074ebb6b89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 02:55]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-43-2457e8bb4065>:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  load_accuracy = load_metric(\"accuracy\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d19c10ec66a143b68b98576fc33ddfd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9284d21715540739b689c6d9320227a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.1864524632692337,\n",
              " 'eval_accuracy': 0.947,\n",
              " 'eval_f1': 0.9475143592790652,\n",
              " 'eval_runtime': 178.2898,\n",
              " 'eval_samples_per_second': 56.088,\n",
              " 'eval_steps_per_second': 3.506,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "7f7UHRCQkm-f",
        "outputId": "2d526972-e86c-42f0-b693-3952d989625f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd7fea66b3894ad6a93b5b972174caff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1712885001.eff7e4a3a353.3061.3:   0%|          | 0.00/457 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/abhiramd22/finetuning-sentiment-model-distilbert-imdb/commit/c002988016178629cc27e60586b62dc451977cc7', commit_message='End of training', commit_description='', oid='c002988016178629cc27e60586b62dc451977cc7', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ptRHhpiXzN68",
        "outputId": "204607dc-87ae-4449-d484-146a3fca1c5c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name '_C' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f690cad06eff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load model directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abhiramd22/finetuning-sentiment-model-mpnet-imdb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torch_pytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_output_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_pytree.Context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
          ]
        }
      ],
      "source": [
        "# from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "# # Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"abhiramd22/finetuning-sentiment-model-mpnet-imdb\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"abhiramd22/finetuning-sentiment-model-mpnet-imdb\")\n",
        "# # define the mappings as dictionaries\n",
        "# label2id = {\n",
        "#     \"LABEL_1\":'POSITIVE',\n",
        "#     \"LABEL_0\":'NEGATIVE'\n",
        "# }\n",
        "# id2label ={\n",
        "#     \"POSITIVE\":'LABEL_1',\n",
        "#     \"NEGATIVE\":'LABEL_0'\n",
        "# }\n",
        "# # define model checkpoint - can be the same model that you already have on the hub\n",
        "# # model_ckpt = ...\n",
        "# # define config\n",
        "# config = AutoConfig.from_pretrained(model, label2id=label2id, id2label=id2label)\n",
        "# # load model with config\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model, config=config)\n",
        "# # export model\n",
        "# # model.save_pretrained(target_name_or_path)\n",
        "# model.push_to_hub(\"sentiment_analysis_pos_neg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c0DKbzc6700",
        "outputId": "5bd69091-f4e9-49f5-e9c5-a3586e99057f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'label': 'POSITIVE', 'score': 0.5457619428634644}, {'label': 'NEGATIVE', 'score': 0.45423799753189087}]]\n"
          ]
        }
      ],
      "source": [
        "# import requests\n",
        "\n",
        "# API_URL = \"https://api-inference.huggingface.co/models/abhiramd22/finetuning-sentiment-model-mpnet-imdb\"\n",
        "# headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "# def query(payload):\n",
        "# \tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "# \treturn response.json()\n",
        "\n",
        "# output = query({\n",
        "# \t\"inputs\": ['test']\n",
        "# })\n",
        "# print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VvHkMBiA0S2"
      },
      "outputs": [],
      "source": [
        "# ouput_formatted=[]\n",
        "# for i in output:\n",
        "#     i[0]['score']=round(i[0]['score']*100,2)\n",
        "#     ouput_formatted.append(i[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdw5nqezCL4I",
        "outputId": "ba1d9cea-e536-4e9a-eca2-fd2134c681dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this will be printed immediately\n",
            "wow\n"
          ]
        }
      ],
      "source": [
        "# # print(ouput_formatted)\n",
        "# import time\n",
        "# def test():\n",
        "#     time.sleep(2)\n",
        "#     print(\"wow\")\n",
        "\n",
        "# from threading import Thread\n",
        "# # test()\n",
        "# Thread(target=test).start()\n",
        "# print(\"this will be printed immediately\")\n",
        "# time.sleep(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1GaC_VJkfzy"
      },
      "source": [
        "# Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "KL04DiT9dzuH",
        "outputId": "6f537331-fe6d-4391-e372-93ed7c5c22ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4023' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4023/5000 12:41 < 03:04, 5.28 it/s, Epoch 1.61/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.414700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.308600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.215200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.225000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.207100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 15:48, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.414700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.308600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.215200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.225000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.207100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.216200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.215900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.2748765350341797, metrics={'train_runtime': 949.1694, 'train_samples_per_second': 84.284, 'train_steps_per_second': 5.268, 'total_flos': 2226643083747840.0, 'train_loss': 0.2748765350341797, 'epoch': 2.0})"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "gS4G0xvzd1tF",
        "outputId": "3da6fa20-c0cb-4d3c-8d63-81b66b429476"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:35]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.290859192609787,\n",
              " 'eval_accuracy': 0.8953,\n",
              " 'eval_f1': 0.8944237168498538,\n",
              " 'eval_runtime': 36.7054,\n",
              " 'eval_samples_per_second': 272.44,\n",
              " 'eval_steps_per_second': 17.027,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "uudAlpYFd3dm",
        "outputId": "6337095e-c55a-4db6-c6ec-3dc9dda57bfd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3e01ae1d9e1402384333153fdfd0683",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1712022046.8c515c8d7ec3.1468.8:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a59ddd93eec4e438eb7028026504364",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b648d3298c04139aee488a945cef9d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db144e545bb74ff18f776fb4b8ccbd13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1712023033.8c515c8d7ec3.1468.9:   0%|          | 0.00/457 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/abhiramd22/finetuning-sentiment-model-mpnet/commit/4eae75b6249d917962b7efe12316a4c35013c053', commit_message='End of training', commit_description='', oid='4eae75b6249d917962b7efe12316a4c35013c053', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mY2jSjSIQX-2",
        "outputId": "4b67f085-12fc-4cee-d85d-8129cab8738e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 08:08, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.469000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.400100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.378600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.374500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.356600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.255000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.261100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.251100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.262300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.250900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.3259083755493164, metrics={'train_runtime': 489.1908, 'train_samples_per_second': 163.535, 'train_steps_per_second': 10.221, 'total_flos': 1121038477976064.0, 'train_loss': 0.3259083755493164, 'epoch': 2.0})"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "gfpq6Ai3VH1o",
        "outputId": "e258a43e-a2bf-4630-b985-fcd6bbec1f90"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 00:17]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-66-2457e8bb4065>:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  load_accuracy = load_metric(\"accuracy\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a738ff9598454cd19e792e3a0b3ac90b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e625ea0060c4784a3c7d0429de2ec12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.3653072118759155,\n",
              " 'eval_accuracy': 0.86,\n",
              " 'eval_f1': 0.8598037252153014,\n",
              " 'eval_runtime': 18.9312,\n",
              " 'eval_samples_per_second': 528.229,\n",
              " 'eval_steps_per_second': 33.014,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "g6AZNdmVQhvV",
        "outputId": "d3e089ac-2a30-471a-c81c-bbd116e6980f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1038100eaf5c4789b518a73bf02af246",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4421a5d809ba41f18e6b6a9f8e07fb8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1712021066.8c515c8d7ec3.1468.6:   0%|          | 0.00/6.93k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cff30d5ddc564acbabb12c4467b07607",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1712021585.8c515c8d7ec3.1468.7:   0%|          | 0.00/457 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/abhiramd22/finetuning-sentiment-model/commit/6b2b53540c89785ba541ef4268567c88e3c7b43a', commit_message='End of training', commit_description='', oid='6b2b53540c89785ba541ef4268567c88e3c7b43a', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H3dq_Z3U5vk"
      },
      "source": [
        "# TRAIN and TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEAOGPorVQvl"
      },
      "outputs": [],
      "source": [
        "dimension_mpnet=768\n",
        "def X_tensor_mpnet(corpus):\n",
        "  corpus=corpus.copy()\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(mpnet_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "\n",
        "X_mpnet=X_tensor_mpnet(rt_reviews_data['reviewText'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yztow75BVwEa"
      },
      "outputs": [],
      "source": [
        "Y=rt_reviews_data['scoreSentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czIuYXORVpMA",
        "outputId": "a55f6b72-959b-4481-d4f6-f3deaec4ac90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average training accuracy is 0.9769\n"
          ]
        }
      ],
      "source": [
        "#A common function to run Neural Network on training data and test data. Uses arguments to change various parameters.\n",
        "    #Kfold\n",
        "\n",
        "# train_X_tensor=X_tensor(train_X)\n",
        "#train\n",
        "nn_model = generateNNModel(0.001, dimension_mpnet,'adam')\n",
        "reset_seeds()\n",
        "train_X=tf.convert_to_tensor(list(X_mpnet), dtype=tf.float32)\n",
        "history = nn_model.fit(train_X, Y_tensor(list(Y)), epochs=15, batch_size=64,verbose=0)\n",
        "\n",
        "#accuracy of training\n",
        "reset_seeds()\n",
        "training_data_predicted_values=nn_model.predict(train_X,verbose=0)\n",
        "training_data_predicted_categories = vec2cat(training_data_predicted_values)\n",
        "\n",
        "train_acc=accuracy_score(Y, training_data_predicted_categories)\n",
        "\n",
        "# #accuracy of validation\n",
        "# reset_seeds()\n",
        "# val_X=tf.convert_to_tensor(val_X, dtype=tf.float32)\n",
        "# validation_data_predicted_values=nn_model.predict(val_X,verbose=0)\n",
        "# validation_data_predicted_categories = vec2cat(validation_data_predicted_values)\n",
        "\n",
        "# val_acc.append(accuracy_score(val_y, validation_data_predicted_categories))\n",
        "\n",
        "print(\"\\nThe average training accuracy is {}\".format(train_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7GV9x-ZXCez"
      },
      "outputs": [],
      "source": [
        "imdb_reviews_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSaniQnmQjFbEPc9-TDwvg0GnSvl0ij_VP-lEM9agQDBQUEFeZ0jzpl6kOCP1jCWhett1WsgDt6NNKU/pub?gid=683699041&single=true&output=csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCELfkLBXVEL"
      },
      "outputs": [],
      "source": [
        "mpnet_encodings_fetched_imdb=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format('imdb'))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format('imdb'),'r') as json_file:\n",
        "        mpnet_encodings_fetched_imdb = json.load(json_file)\n",
        "# else:\n",
        "#     # Load all-mpnet-base-v2 model\n",
        "#     mpnet_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "#     clear_output()\n",
        "#     def X_tensor_mpnet_original(corpus):\n",
        "#         corpus=corpus.copy()\n",
        "#         X_train = mpnet_model.encode(corpus)\n",
        "#         X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "#         return X_train\n",
        "\n",
        "#     input_converted_mpnet=X_tensor_mpnet_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "#     mpnet_encodings_fetched_imdb={}\n",
        "\n",
        "#     for i in range(rt_reviews_data.shape[0]):\n",
        "#         mpnet_encodings_fetched_imdb[rt_reviews_data['reviewText'][i]]=input_converted_mpnet[i].numpy().tolist()\n",
        "\n",
        "#     with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format(imdb),'w') as fp:\n",
        "#         fp.write(json.dumps(mpnet_encodings_fetched_imdb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "rf7pU57EY-5J",
        "outputId": "c70132b4-3aa4-4cb7-fa77-d1ddc5ca568e"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'scoreSentiment'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'scoreSentiment'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-284a33aaf94c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mset_of_list_categories_imdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_reviews_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_reviews_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scoreSentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_of_list_categories_imdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mnewarray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_of_list_categories_imdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'scoreSentiment'"
          ]
        }
      ],
      "source": [
        "# Create Dictionary to convert category into 1-hot vector and vice verse\n",
        "category_to_vector_imdb={}\n",
        "vector_to_category_imdb={}\n",
        "\n",
        "index=0\n",
        "set_of_list_categories_imdb=set(list(imdb_reviews_data['scoreSentiment']))\n",
        "set(list(imdb_reviews_data['scoreSentiment']))\n",
        "for i in list(set_of_list_categories_imdb):\n",
        "  newarray=[0]*len(set_of_list_categories_imdb)\n",
        "  newarray[index]=1\n",
        "  category_to_vector_imdb[i]=newarray\n",
        "  vector_to_category_imdb[index]=i\n",
        "  index+=1\n",
        "\n",
        "print(category_to_vector_imdb)\n",
        "print(vector_to_category_imdb)\n",
        "\n",
        "# utility functions to convert category into 1-hot vector and vice verse\n",
        "def Y_tensor(Y_train):\n",
        "  Y_train=Y_train.copy()\n",
        "\n",
        "  for i in range(len(Y_train)):\n",
        "    Y_train[i]=category_to_vector_imdb[Y_train[i]]\n",
        "\n",
        "  Y_train=np.array(Y_train)\n",
        "  Y_train=tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "  return Y_train\n",
        "\n",
        "def vec2cat(input):\n",
        "    categories = []\n",
        "    for i in range(len(input)):\n",
        "        categories.append(vector_to_category_imdb[np.argmax(input[i])])\n",
        "    return categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsXRrd3fXlP_",
        "outputId": "e0a8f5e0-f2d1-4e6e-ff63-2191416317b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The average testing accuracy is 0.0\n"
          ]
        }
      ],
      "source": [
        "def X_tensor_mpnet_imdb(corpus):\n",
        "  corpus=corpus.copy()\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(mpnet_encodings_fetched_imdb[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "X_mpnet_imdb=X_tensor_mpnet_imdb(imdb_reviews_data['review'])\n",
        "train_X=tf.convert_to_tensor(list(X_mpnet_imdb), dtype=tf.float32)\n",
        "#accuracy of training\n",
        "reset_seeds()\n",
        "training_data_predicted_values=nn_model.predict(train_X,verbose=0)\n",
        "training_data_predicted_categories = vec2cat_imdb(training_data_predicted_values)\n",
        "\n",
        "test_acc=accuracy_score(imdb_reviews_data['sentiment'], training_data_predicted_categories)\n",
        "\n",
        "print(\"\\nThe average testing accuracy is {}\".format(test_acc))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rLynEWx6NEiK",
        "q6Q5rFNBYYNf",
        "RCNwxieEYopR",
        "ezEhrPeIlbRq",
        "-jCKm7eQyUrY",
        "ADnnvdZ2KMc3",
        "6mjnOwK8KNsv",
        "nP-JBJOfJwuU",
        "yw2IDx9Xyc7T",
        "Lr0BpG5iyamL",
        "p7jOY_RNlIw5",
        "O2SqBbnlYkDO",
        "iQ36Yzvn7JUB",
        "sMw-IgLTOzvE",
        "U1GaC_VJkfzy",
        "2H3dq_Z3U5vk"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}